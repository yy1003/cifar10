{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "このnotebookでは，実際にモデルを学習していきます．   \n",
    "パラメータは，記事と同一ですが，初期値やAugmentationの関係で記事と同じにはなりません．   \n",
    "GPU環境でも，一つのモデルの学習に1.5時間程度かかります．   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業ディレクトリの指定\n",
    "以下のcellで，data_dirに作業をするディレクトリを末尾の\"/\"まで含めて指定してください.   \n",
    "data_dir 直下には以下の5つのディレクトリを作成してください\n",
    "- comparisons (historyの比較結果の保存)\n",
    "- histories (historyの保存)\n",
    "- loss_and_acc (lossとaccの視覚化結果の保存)\n",
    "- models (学習したmodelの保存)\n",
    "- predictions (予測結果の保存)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"hoge/\" #作業をするディレクトリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install keras\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import  Model,load_model\n",
    "from keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Input,GlobalAveragePooling2D,BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの評価と可視化に用いる関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modelの評価をする関数. lossとaccuracyを標準出力\n",
    "def my_eval(model,x,t):\n",
    "    #model: 評価したいモデル, x: 予測する画像 shape = (batch,32,32,3) t:one-hot表現のlabel\n",
    "    ev = model.evaluate(x,t)\n",
    "    print(\"loss:\" ,end = \" \")\n",
    "    print(ev[0])\n",
    "    print(\"acc: \", end = \"\")\n",
    "    print(ev[1])\n",
    "\n",
    "\n",
    "## historyの可視化．nameを指定した場合は historiesに引数として与えたhistory， loss_and_accに可視化結果をそれぞれ保存\n",
    "def loss_and_acc(history,file_name = None):\n",
    "    fig,ax = plt.subplots(1,2,figsize = (10,5))\n",
    "    epochs = len(history.history[\"loss\"])\n",
    "    ax[0].plot(range(epochs), history.history[\"loss\"],label = \"train_loss\",c = \"red\")\n",
    "    ax[0].plot(range(epochs), history.history[\"val_loss\"],label = \"valid_loss\",c = \"green\")\n",
    "    ax[0].set_xlabel(\"epochs\",fontsize = 14)\n",
    "    ax[0].set_ylabel(\"loss\",fontsize  = 14)\n",
    "    ax[0].legend(fontsize = 14)\n",
    "\n",
    "    ax[1].plot(range(epochs), history.history[\"acc\"],label = \"train_acc\",c = \"red\")\n",
    "    ax[1].plot(range(epochs), history.history[\"val_acc\"],label = \"valid_acc\",c = \"green\")\n",
    "    ax[1].set_xlabel(\"epochs\",fontsize = 14)\n",
    "    ax[1].set_ylabel(\"acc\",fontsize = 14)\n",
    "    ax[1].legend(fontsize = 14)\n",
    "    if(file_name != None):\n",
    "        with open(data_dir + \"histories/\" + file_name + \".binaryfile\",mode = \"wb\") as f:\n",
    "            pickle.dump(history,f)\n",
    "    if(file_name != None):\n",
    "        fig.savefig(data_dir + \"loss_and_acc/\" + file_name + \"_acc\" )\n",
    "\n",
    "\n",
    "## historyのロード  loss_and_acc()で与えたfile_nameを指定すると，そのhistoryをロードして返り値とする\n",
    "def load_history(file_name):\n",
    "    with open(data_dir + \"histories/\" + file_name + \".binaryfile\",mode = \"rb\") as f:\n",
    "        res = pickle.load(f)\n",
    "    return res\n",
    "\n",
    "\n",
    "## 2つのhistoryの比較と保存\n",
    "def compare(his1,his1_name,his2,his2_name, file_name = None):\n",
    "    #his1: 比較したいヒストリー , his1_name: his1のラベル   his2も同様\n",
    "    #file_name  与えると，可視化結果を保存\n",
    "    keys = [\"loss\",\"val_loss\",\"acc\",\"val_acc\"]\n",
    "    fig, ax = plt.subplots(2,2,figsize = (12,12))\n",
    "    epochs = min( [len(his1.history[\"loss\"]), len(his2.history[\"loss\"])] )\n",
    "\n",
    "    ind = 0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax[i,j].plot(range(epochs),his1.history[keys[ind]][:epochs],label = his1_name)\n",
    "            ax[i,j].plot(range(epochs),his2.history[keys[ind]][:epochs],label = his2_name)\n",
    "            ax[i,j].set_xlabel(\"epochs\",fontsize = 14)\n",
    "            ax[i,j].set_ylabel(keys[ind],fontsize = 14)\n",
    "            ax[i,j].legend(fontsize = 14)\n",
    "\n",
    "            ind += 1\n",
    "\n",
    "    if(file_name != None):\n",
    "        fig.savefig(data_dir + \"comparisons/\" + file_name + \"_comp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルを作成する関数\n",
    "ベンチマークとして，比較的小さい規模のモデルを返り値とする`create_bench_model`と，   \n",
    "深い層の`create_deep_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bench_model():\n",
    "    inputs = Input(shape = (32,32,3))\n",
    "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(inputs)\n",
    "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = Dense(1024,activation = \"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    y = Dense(10,activation = \"softmax\")(x)\n",
    "    return Model(input = inputs, output = y)\n",
    "\n",
    "\n",
    "def create_deep_model():\n",
    "    inputs = Input(shape = (32,32,3))\n",
    "\n",
    "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(inputs)\n",
    "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = Conv2D(256,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = Conv2D(512,(3,3),padding = \"SAME\",activation= \"relu\")(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = Dense(1024,activation = \"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024,activation = \"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y  = Dense(10,activation = \"softmax\")(x)\n",
    "    return Model(inputs,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各種手法で用いる関数\n",
    "- Data Augmentation\n",
    "- step decay\n",
    "- TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def da_generator():\n",
    "    return ImageDataGenerator(rotation_range = 20, horizontal_flip = True, height_shift_range = 0.2,\\\n",
    "                                width_shift_range = 0.2,zoom_range = 0.2, channel_shift_range = 0.2\n",
    "                                ).flow(x_train,t_train, batch_size )\n",
    "\n",
    "\n",
    "def da_generator_strong():\n",
    "    return ImageDataGenerator(rotation_range = 20, horizontal_flip = True, height_shift_range = 0.3,\\\n",
    "                                width_shift_range = 0.3,zoom_range = 0.3, channel_shift_range = 0.3\n",
    "                                ).flow(x_train,t_train, batch_size )\n",
    "\n",
    "def step_decay(epoch):\n",
    "    lr = 0.001\n",
    "    if(epoch >= 100):\n",
    "        lr/=5\n",
    "    if(epoch>=140):\n",
    "        lr/=2\n",
    "    return lr\n",
    "\n",
    "\n",
    "def tta(model,test_size,generator,batch_size ,epochs = 10):\n",
    "    #test_time_augmentation\n",
    "    #batch_sizeは，test_sizeの約数でないといけない．\n",
    "    pred = np.zeros(shape = (test_size,10), dtype = float)\n",
    "    step_per_epoch = test_size //batch_size\n",
    "    for epoch in range(epochs):\n",
    "        for step in range(step_per_epoch):\n",
    "            sta = batch_size * step\n",
    "            end = sta + batch_size\n",
    "            tmp_x = generator.__next__()\n",
    "            pred[sta:end] += model.predict(tmp_x)\n",
    "\n",
    "    return pred / epochs\n",
    "\n",
    "\n",
    "def tta_generator():\n",
    "    return ImageDataGenerator(rotation_range = 20 , horizontal_flip = True,height_shift_range = 0.2,\\\n",
    "                                 width_shift_range = 0.2,zoom_range = 0.2,channel_shift_range = 0.2\\\n",
    "                                  ).flow(x_test,batch_size = batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データのロードと正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 16s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train_raw, t_train_raw), (x_test_raw,t_test_raw) = cifar10.load_data()\n",
    "t_train = to_categorical(t_train_raw)\n",
    "t_test = to_categorical(t_test_raw)\n",
    "x_train = x_train_raw / 255\n",
    "x_test  = x_test_raw / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:10000]\n",
    "t_train = t_train[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチサイズとエポックの決定\n",
    "batch_sizeは，訓練データとテストデータの枚数の公約数にしないと，TTAでバグるかも"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "epochs = 20\n",
    "steps_per_epoch = x_train.shape[0] // batch_size\n",
    "validation_steps = x_test.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ベンチマーク\n",
    "層の浅いモデルで学習・予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_bench_model()\n",
    "model.compile(loss = \"categorical_crossentropy\",optimizer = Adam(), metrics = [\"accuracy\"])\n",
    "train_gen = ImageDataGenerator().flow(x_train,t_train, batch_size )\n",
    "val_gen = ImageDataGenerator().flow(x_test,t_test, batch_size)\n",
    "history = model.fit_generator(train_gen, epochs=epochs, steps_per_epoch = steps_per_epoch,\\\n",
    "                          validation_data = val_gen, validation_steps =validation_steps)\n",
    "\n",
    "### 評価と保存\n",
    "my_eval(model,x_test,t_test)\n",
    "loss_and_acc(history,\"bench\")\n",
    "model.save(data_dir + \"models/bench.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Data Augmentationで汎化性能向上を目指す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_bench_model()\n",
    "model.compile(loss = \"categorical_crossentropy\",optimizer = Adam(), metrics = [\"accuracy\"])\n",
    "val_gen = ImageDataGenerator().flow(x_test,t_test, batch_size)\n",
    "history = model.fit_generator(da_generator(), epochs=epochs, steps_per_epoch = steps_per_epoch,\\\n",
    "                          validation_data = val_gen, validation_steps = validation_steps)\n",
    "\n",
    "###評価と保存\n",
    "my_eval(model,x_test,t_test)\n",
    "loss_and_acc(history,\"DA\")\n",
    "bench_history = load_history(\"bench\")\n",
    "compare(bench_history,\"no_DA\",history,\"DA\",\"no_DA_vs_DA\")\n",
    "model.save(data_dir + \"models/DA.hdf5\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 層を増やす\n",
    "層を増やすことにより，モデルの表現力向上を目指す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_deep_model()\n",
    "model.compile(loss = \"categorical_crossentropy\",optimizer = Adam(), metrics = [\"accuracy\"])\n",
    "val_gen = ImageDataGenerator().flow(x_test,t_test, batch_size)\n",
    "history = model.fit_generator(da_generator(), epochs=epochs, steps_per_epoch = steps_per_epoch,\\\n",
    "                          validation_data = val_gen, validation_steps = validation_steps)\n",
    "\n",
    "###評価と保存\n",
    "my_eval(model,x_test,t_test)\n",
    "loss_and_acc(history,\"deep\")\n",
    "bench_history = load_history(\"DA\")\n",
    "compare(bench_history,\"bench\",history,\"deep\",\"bench_vs_deep\")\n",
    "model.save(data_dir + \"models/deep.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習率減衰\n",
    "学習率減衰させることにより，パラメータを細かく調整する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_deep_model()\n",
    "model.compile(loss = \"categorical_crossentropy\",optimizer = Adam(), metrics = [\"accuracy\"])\n",
    "val_gen = ImageDataGenerator().flow(x_test,t_test, batch_size)\n",
    "lr_decay = LearningRateScheduler(step_decay)\n",
    "history = model.fit_generator(da_generator(), epochs=epochs, steps_per_epoch = steps_per_epoch,\\\n",
    "                        validation_data = val_gen, validation_steps = validation_steps,callbacks = [lr_decay])\n",
    "\n",
    "###評価と保存\n",
    "my_eval(model,x_test,t_test)\n",
    "loss_and_acc(history,\"lr_decay\")\n",
    "no_decay_history = load_history(\"deep\")\n",
    "compare(no_decay_history,\"no_decay\",history,\"decay\",\"no_decay_vs_decay\")\n",
    "model.save(data_dir + \"models/lr_decay.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time Augmentation\n",
    "予測時にAugmentationを行うことで，精度の向上を目指す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_epochs = 5\n",
    "model = load_model(data_dir + \"models/lr_decay.hdf5\")\n",
    "tta_pred = tta(model,x_test.shape[0],tta_generator(),batch_size ,epochs = tta_epochs)\n",
    "\n",
    "print(\"tta_acc: \",end = \"\")\n",
    "print( accuracy_score( np.argmax(tta_pred,axis = 1) , np.argmax(t_test,axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# アンサンブル学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アンサンブル学習で精度の向上を目指す   \n",
    "多様性を持たせるため半分のモデルはより強いdata augmentationで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_epochs = 2\n",
    "tta_epochs = 5\n",
    "\n",
    "for i in range(ens_epochs):\n",
    "    model = create_deep_model()\n",
    "    model.compile(loss = \"categorical_crossentropy\",optimizer = Adam(), metrics = [\"accuracy\"])\n",
    "    val_gen = ImageDataGenerator().flow(x_test,t_test, batch_size)   \n",
    "    lr_decay = LearningRateScheduler(step_decay)\n",
    "\n",
    "    if(i < ens_epochs/2 ):\n",
    "        train_gen = da_generator()\n",
    "    else:\n",
    "        train_gen = da_generator_strong()\n",
    "    his = model.fit_generator(train_gen, epochs=epochs, steps_per_epoch = steps_per_epoch,\\\n",
    "                          validation_data = val_gen, validation_steps = validation_steps,verbose = 0,callbacks = [lr_decay])\n",
    "\n",
    "    pred = tta(model,x_test.shape[0],tta_generator(),batch_size ,epochs = tta_epochs)\n",
    "    np.save(data_dir + \"predictions/\" + \"pred_\" + str(i),pred)\n",
    "    model.save(data_dir + \"models/ensemble_\" +str(i) +\".hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acc_meanは，各モデル単体で予測した時のaccuracyの平均値   \n",
    "final_accは，各モデルの予測の平均値のaccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "final_pred = np.zeros_like(t_test)\n",
    "for i in range(ens_epochs):\n",
    "    pred = np.load(data_dir + \"predictions/pred_\" + str(i) + \".npy\")\n",
    "    acc_list.append(accuracy_score( np.argmax(pred,axis = 1), np.argmax(t_test,axis = 1)) )\n",
    "    final_pred += pred\n",
    "\n",
    "final_pred /= ens_epochs\n",
    "np.save(data_dir + \"predictions/final_pred\",final_pred)\n",
    "print(\"acc_mean: \",end = \"\")\n",
    "print( np.mean(acc_list))\n",
    "print(\"final_acc: \" ,end = \"\")\n",
    "print( accuracy_score(np.argmax(final_pred,axis = 1), np.argmax(t_test,axis = 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
